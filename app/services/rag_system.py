import re
import logging
import time
import json
import os
from typing import List, Dict, Any, Optional, Tuple

# Importer les services existants
from app.services.embedding_service import EmbeddingService
from app.services.milvus_service import MilvusService
from app.services.search_service import SearchService
from app.services.rerank_service import RerankService
from app.services.llm_service import LlmService
from fastapi import FastAPI

# Configuration du logger
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

app = FastAPI()

class RAGSystem:
    """
    Syst√®me RAG complet pour le domaine juridique camerounais.
    Int√®gre la recherche, le reranking, la r√©organisation du contexte et la g√©n√©ration de r√©ponses.
    """
    def __init__(
        self, 
        collection_name: str = "documents_collection", 
        embedding_dim: int = 1024,
        top_k: int = 8,
        llm_model: str = "llama3.2:latest",
        max_context_length: int = 4000,
        save_dir: str = "./rag_results"
    ):
        """
        Initialise le syst√®me RAG avec tous ses composants.
        
        Args:
            collection_name: Nom de la collection Milvus
            embedding_dim: Dimension des embeddings
            top_k: Nombre de r√©sultats √† retourner lors de la recherche initiale
            llm_model: Nom du mod√®le LLM √† utiliser
            max_context_length: Longueur maximale du contexte envoy√© au LLM
            save_dir: R√©pertoire pour sauvegarder les r√©sultats
        """
        # Cr√©er le r√©pertoire de sauvegarde s'il n'existe pas
        os.makedirs(save_dir, exist_ok=True)
        self.save_dir = save_dir
        self.top_k = top_k
        self.max_context_length = max_context_length

        # Initialisation des services
        try:
            # Services de base
            logger.info("üöÄ Initialisation des services...")
            self.embedding_service = EmbeddingService()
            self.milvus_service = MilvusService(
                collection_name=collection_name, 
                dim=embedding_dim
            )
            logger.info("‚úÖ Services d'embedding et Milvus initialis√©s")

            # Service de recherche
            self.search_service = SearchService(
                milvus_service=self.milvus_service,
                embedding_service=self.embedding_service,
                top_k=top_k
            )
            logger.info("‚úÖ Service de recherche initialis√©")

            # Service LLM
            try:
                self.llm_service = LlmService(model_name=llm_model)
                self.llm_available = True
                logger.info("‚úÖ Service LLM initialis√©")
            except Exception as llm_error:
                logger.warning(f"‚ö†Ô∏è Service LLM non disponible: {llm_error}")
                self.llm_service = None
                self.llm_available = False

            # Service de reranking
            self.rerank_service = RerankService(
                llm_service=self.llm_service,
                cache_size=100
            )
            logger.info("‚úÖ Service de reranking initialis√©")

        except Exception as e:
            logger.error(f"‚ùå Erreur lors de l'initialisation des services: {e}")
            raise RuntimeError(f"Impossible d'initialiser les services : {e}")

    def detect_query_type(self, query: str) -> str:
        """
        D√©tecte le type de requ√™te juridique.
        
        Args:
            query: Requ√™te de l'utilisateur
            
        Returns:
            Type de requ√™te identifi√©
        """
        query_lower = query.lower()
        
        # D√©tection de recherche d'articles sp√©cifiques
        if re.search(r"article\s+\d+", query_lower) or any(pattern in query_lower for pattern in ["article", "loi", "constitution"]):
            return "article"
        
        # D√©tection de demande de d√©finition
        if any(term in query_lower for term in ["qu'est-ce que", "d√©finition", "signifie", "d√©finir"]):
            return "definition"
        
        # D√©tection de demande de proc√©dure
        if any(term in query_lower for term in ["comment", "proc√©dure", "processus", "d√©marche", "√©tape"]):
            return "procedure"
        
        # D√©tection de requ√™te fiscale
        if any(term in query_lower for term in ["imp√¥t", "taxe", "fiscal", "tva", "irpp", "revenu"]):
            return "fiscal"
        
        # Par d√©faut, requ√™te g√©n√©rale
        return "general"

    def format_context_for_prompt(self, reranked_results: List[Dict[str, Any]]) -> str:
        """
        Formate les r√©sultats r√©organis√©s en un contexte structur√© pour le LLM.
        
        Args:
            reranked_results: Liste de r√©sultats reclass√©s et r√©organis√©s
            
        Returns:
            Contexte format√© pour le prompt LLM
        """
        if not reranked_results:
            return "Aucune information pertinente trouv√©e."
        
        # Regrouper par document pour une meilleure organisation
        document_groups = {}
        for result in reranked_results:
            metadata = result.get("metadata", {})
            doc_id = metadata.get("document_id", "inconnu")
            filename = metadata.get("filename", "Document inconnu")
            page = metadata.get("page_number", "?")
            
            doc_key = f"{filename} (ID: {doc_id})"
            if doc_key not in document_groups:
                document_groups[doc_key] = []
            
            document_groups[doc_key].append({
                "text": result.get("text", ""),
                "page": page,
                "score": result.get("rerank_score", result.get("score", 0.0))
            })
        
        # Formater le contexte
        context_parts = []
        
        for doc_name, extracts in document_groups.items():
            # Ajouter le titre du document
            context_parts.append(f"### {doc_name}")
            
            # Trier les extraits par score (meilleurs d'abord)
            sorted_extracts = sorted(extracts, key=lambda x: x["score"], reverse=True)
            
            # Ajouter chaque extrait
            for extract in sorted_extracts:
                context_parts.append(f"[Page {extract['page']}] {extract['text']}")
            
            # Ajouter un s√©parateur entre les documents
            context_parts.append("---")
        
        return "\n\n".join(context_parts)

    def _create_prompt(self, query: str, context: str) -> str:
        """
        M√©thode originale de cr√©ation de prompt.
        Conserv√©e pour compatibilit√© et comme m√©thode de secours.
        
        Args:
            query: Question de l'utilisateur
            context: Contexte format√© avec les informations pertinentes
            
        Returns:
            Prompt complet pour le LLM
        """
        # Base du prompt
        prompt = f"""En tant qu'expert juridique camerounais, je vais r√©pondre √† la question suivante 
    en me basant uniquement sur les extraits de documents fournis:

    QUESTION: {query}

    EXTRAITS DE DOCUMENTS PERTINENTS:
    {context}

    """
        
        # Instructions g√©n√©riques par d√©faut
        prompt += """INSTRUCTIONS:
    - R√©pondez de mani√®re claire, concise et pr√©cise
    - Utilisez uniquement les informations pr√©sentes dans les extraits
    - Citez les sources pertinentes (document, page)
    - Indiquez si la r√©ponse est partielle ou incompl√®te
    - Structurez votre r√©ponse de mani√®re logique et compr√©hensible
    """
        
        prompt += "\nR√âPONSE:"
        
        return prompt

    def search_and_rerank(
        self, 
        query: str, 
        use_expansion: bool = True, 
        use_reranking: bool = True
    ) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:
        """
        Effectue la recherche et le reranking pour une requ√™te.
        
        Args:
            query: Requ√™te de l'utilisateur
            use_expansion: Utiliser l'expansion de requ√™te
            use_reranking: Appliquer le reranking aux r√©sultats
            
        Returns:
            Tuple de (r√©sultats reclass√©s, statistiques)
        """
        start_time = time.time()
        top_k = 5

        # Recherche avec ou sans expansion
        search_results = (
            self.search_service.search_with_expansion(query, top_k=top_k) 
            if use_expansion 
            else self.search_service.search(query, top_k=top_k)
        )
        
        # Initialiser les statistiques
        stats = {
            "timestamp": start_time,
            "use_expansion": use_expansion,
            "use_reranking": use_reranking
        }
        
        try:
            # Recherche avec ou sans expansion
            search_start = time.time()
            logger.info(f"üîç Recherche pour la requ√™te: '{query}'")
            
            search_results = (
                self.search_service.search_with_expansion(query) 
                if use_expansion 
                else self.search_service.search(query)
            )
            
            search_time = time.time() - search_start
            stats["search_time"] = search_time
            stats["raw_results_count"] = len(search_results)
            
            logger.info(f"‚úÖ Recherche termin√©e en {search_time:.3f}s, {len(search_results)} r√©sultats trouv√©s")
            
            # V√©rifier si des r√©sultats ont √©t√© trouv√©s
            if not search_results:
                logger.warning("‚ö†Ô∏è Aucun r√©sultat trouv√© pour cette requ√™te")
                stats["total_time"] = time.time() - start_time
                return [], stats
            
            # Reranking si demand√©
            if use_reranking and self.rerank_service:
                rerank_start = time.time()
                logger.info(f"üìä Reranking de {len(search_results)} r√©sultats...")
                
                try:
                    reranked_results = self.rerank_service.rerank(
                        query=query,
                        results=search_results,
                        use_llm=False
                    )
                    
                    rerank_time = time.time() - rerank_start
                    stats["rerank_time"] = rerank_time
                    stats["total_time"] = time.time() - start_time
                    
                    logger.info(f"‚úÖ Reranking termin√© en {rerank_time:.3f}s")
                    
                    return reranked_results[:self.top_k], stats
                    
                except Exception as e:
                    logger.error(f"‚ùå Erreur lors du reranking: {e}")
                    # Fallback aux r√©sultats originaux
                    return search_results[:self.top_k], stats
            else:
                # Retourner les r√©sultats de recherche originaux
                return search_results[:self.top_k], stats
                
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la recherche et du reranking: {e}")
            return [], stats

    def generate_answer(self, query: str) -> Dict[str, Any]:
        try:
            # Recherche et reranking
            results, search_stats = self.search_and_rerank(query)
            
            if not results:
                return {
                    "query": query,
                    "answer": "Je n'ai pas trouv√© d'informations pertinentes.",
                    "source_documents": [],
                    "stats": search_stats,
                    "success": False
                }
            
            # D√©tecter le type de requ√™te
            query_type = self.detect_query_type(query)
            
            # Formater le contexte
            context = self.format_context_for_prompt(results)
            
            # S√©lectionner dynamiquement le meilleur prompt
            prompt = self._select_best_prompt(query, context, query_type)
            
            try:
                # Tentative de g√©n√©ration LLM avec timeout et retry
                llm_start = time.time()
                response = self._generate_llm_response_with_fallback(prompt, results)
                
                return {
                    "query": query,
                    "answer": response,
                    "source_documents": results[:5],
                    "stats": {
                        **search_stats,
                        "llm_generation_time": time.time() - llm_start,
                        "query_type": query_type
                    },
                    "success": True
                }
            
            except Exception as e:
                # Fallback : g√©n√©rer une r√©ponse √† partir des extraits
                logging.warning(f"Fallback activ√© : {e}")
                summary_response = self._generate_summary_from_extracts(results)
                
                return {
                    "query": query,
                    "answer": summary_response,
                    "source_documents": results[:5],
                    "stats": search_stats,
                    "success": False,
                    "error": str(e)
                }
        
        except Exception as e:
            logging.error(f"Erreur globale : {e}")
            return {
                "query": query,
                "answer": "Une erreur est survenue lors du traitement de votre requ√™te.",
                "source_documents": [],
                "stats": {},
                "success": False,
                "error": str(e)
            }

    def _generate_source_explanation(self, results: List[Dict]) -> str:
        """
        G√©n√®re une explication sur les sources des informations.
        """
        if not results:
            return "Aucune source n'a pu √™tre identifi√©e."
        
        source_info = []
        unique_docs = set()
        
        for result in results[:3]:  # Limiter aux 3 premi√®res sources
            metadata = result.get('metadata', {})
            filename = metadata.get('filename', 'Document inconnu')
            page = metadata.get('page_number', '?')
            document_id = metadata.get('document_id', '')
            
            if document_id not in unique_docs:
                unique_docs.add(document_id)
                source_info.append(f"- {filename} (page {page})")
        
        return f"Informations extraites de : \n" + "\n".join(source_info)
        
    def _generate_llm_response_with_fallback(self, prompt: str, results: List[Dict]) -> str:
        # Tentative de g√©n√©ration avec plusieurs m√©thodes
        methods = [
            lambda: self.llm_service.generate_response(prompt, timeout=120),
            lambda: self.llm_service.generate_response(prompt, max_length=300, timeout=60),
            lambda: self._generate_summary_from_extracts(results)
        ]
        
        for method in methods:
            try:
                response = method()
                if response and len(response) >= 20:
                    return response
            except Exception as e:
                logging.warning(f"M√©thode de g√©n√©ration √©chou√©e : {e}")
        
        return "Je n'ai pas pu g√©n√©rer de r√©ponse d√©taill√©e."

    def _generate_summary_from_extracts(self, results: List[Dict]) -> str:
        # G√©n√©ration d'un r√©sum√© √† partir des extraits
        summary_parts = []
        for result in results[:3]:  # Limiter aux 3 premiers r√©sultats
            text = result.get('text', '')
            if text and len(text) > 50:
                summary_parts.append(f"- {text[:200]}...")
        
        return "R√©sum√© des extraits pertinents :\n" + "\n".join(summary_parts)
    
    def handle_conversation(
        self, 
        message: str, 
        conversation_history: List[Dict[str, str]] = None
    ) -> Dict[str, Any]:
        """
        G√®re une conversation compl√®te avec historique et contexte.
        
        Args:
            message: Message de l'utilisateur
            conversation_history: Historique des conversations pr√©c√©dentes
        
        Returns:
            Dictionnaire avec la r√©ponse, les sources, et les explications
        """
        if conversation_history is None:
            conversation_history = []
        
        # D√©tecter le type de requ√™te
        query_type = self.detect_query_type(message)
        
        # Adapter le prompt en fonction du contexte de conversation
        if query_type == "clarification" and conversation_history:
            # Si c'est une demande de clarification, utiliser le contexte pr√©c√©dent
            last_query = conversation_history[-1].get('query', '')
            message = f"Peux-tu m'expliquer plus en d√©tail la r√©ponse pr√©c√©dente √† propos de : {last_query}"
        
        # Recherche et reranking
        results, search_stats = self.search_and_rerank(message)
        
        if not results:
            return {
                "query": message,
                "answer": "Je n'ai pas trouv√© d'informations pertinentes sur ce sujet.",
                "explanation": "Ma recherche dans les documents juridiques n'a pas permis de trouver des informations correspondant √† votre requ√™te.",
                "source_documents": [],
                "stats": search_stats,
                "success": False
            }
        
        # Formater les documents sources de mani√®re s√©rialisable
        formatted_results = self._format_source_documents(results)
        
        # Formater le contexte
        context = self.format_context_for_prompt(formatted_results)
        
        # Cr√©er un prompt adaptatif
        prompt = self._create_adaptive_prompt(message, context, query_type)
        
        try:
            # G√©n√©ration de la r√©ponse
            response = self._generate_llm_response_with_fallback(prompt, formatted_results)
            
            # G√©n√©rer une explication sur la source des informations
            source_explanation = self._generate_source_explanation(formatted_results)
            
            return {
                "query": message,
                "answer": response,
                "explanation": source_explanation,
                "source_documents": formatted_results[:3],  # Limiter aux 3 premi√®res sources
                "stats": search_stats,
                "success": True
            }
        
        except Exception as e:
            return {
                "query": message,
                "answer": "Je n'ai pas pu g√©n√©rer de r√©ponse compl√®te.",
                "explanation": f"Une erreur est survenue lors de la g√©n√©ration de la r√©ponse : {str(e)}",
                "source_documents": [],
                "stats": search_stats,
                "success": False
            }

    def _create_adaptive_prompt(self, query: str, context: str, query_type: str) -> str:
        """
        Cr√©e un prompt adaptatif en fonction du type de requ√™te.
        
        Args:
            query: Requ√™te de l'utilisateur
            context: Contexte format√©
            query_type: Type de requ√™te d√©tect√©
        
        Returns:
            Prompt adapt√© au type de requ√™te
        """
        base_prompt = f"""En tant qu'expert juridique camerounais, je vais r√©pondre √† la question suivante 
    en me basant uniquement sur les extraits de documents fournis:

    QUESTION: {query}

    EXTRAITS DE DOCUMENTS PERTINENTS:
    {context}

    """
        
        # Instructions sp√©cifiques selon le type de requ√™te
        if query_type == "article":
            base_prompt += """INSTRUCTIONS SP√âCIFIQUES POUR LES ARTICLES:
    - Citez pr√©cis√©ment le texte de l'article demand√© s'il est pr√©sent dans les documents
    - Expliquez clairement la signification et l'importance de cet article
    - Mentionnez la source exacte (document, page) des informations
    - Donnez un contexte d√©taill√© et compr√©hensible
    - Si l'article n'est pas int√©gralement pr√©sent, indiquez-le clairement
    """
        elif query_type == "definition":
            base_prompt += """INSTRUCTIONS SP√âCIFIQUES POUR LES D√âFINITIONS:
    - Fournir une d√©finition pr√©cise et compl√®te du terme juridique
    - Expliquez le contexte et l'importance de ce terme dans le droit camerounais
    - Citez les sources pertinentes des d√©finitions
    - Rendez la d√©finition accessible et claire
    - Mentionnez les implications l√©gales si pertinent
    """
        elif query_type == "procedure":
            base_prompt += """INSTRUCTIONS SP√âCIFIQUES POUR LES PROC√âDURES:
    - D√©crivez les √©tapes de la proc√©dure de mani√®re chronologique et d√©taill√©e
    - Identifiez les documents requis, les d√©lais et les autorit√©s comp√©tentes
    - Expliquez chaque √©tape en termes simples et pratiques
    - Fournissez des conseils ou des points d'attention importants
    - Citez les sources r√©glementaires de chaque √©tape
    """
        elif query_type == "fiscal":
            base_prompt += """INSTRUCTIONS SP√âCIFIQUES POUR LES QUESTIONS FISCALES:
    - D√©taillez pr√©cis√©ment les aspects fiscaux mentionn√©s
    - Expliquez les taux, exon√©rations et obligations fiscales
    - Donnez des exemples concrets et pratiques
    - Situez les dispositions dans le contexte fiscal camerounais
    - Indiquez les r√©f√©rences l√©gales sp√©cifiques
    """
        else:
            # Utiliser les instructions g√©n√©riques si aucun type sp√©cifique n'est d√©tect√©
            base_prompt += """INSTRUCTIONS G√âN√âRIQUES:
    - R√©pondez de mani√®re claire, concise et pr√©cise
    - Utilisez uniquement les informations des extraits fournis
    - Rendez votre r√©ponse accessible et compr√©hensible
    - Donnez un aper√ßu contextuel si possible
    - Citez les sources de vos informations
    """
        
        base_prompt += "\nR√âPONSE D√âTAILL√âE:"
        
        return base_prompt

    def _select_best_prompt(self, query: str, context: str, query_type: str) -> str:
        """
        S√©lectionne dynamiquement le meilleur prompt en fonction du contexte.
        
        Args:
            query: Requ√™te de l'utilisateur
            context: Contexte format√©
            query_type: Type de requ√™te d√©tect√©
        
        Returns:
            Prompt le plus appropri√©
        """
        try:
            # Tenter d'abord le prompt adaptatif
            adaptive_prompt = self._create_adaptive_prompt(query, context, query_type)
            
            # V√©rifier si le prompt adaptatif est suffisamment long et informatif
            if len(adaptive_prompt) > 200:  # S'assurer qu'il contient des instructions substantielles
                return adaptive_prompt
            
            # Fallback au prompt g√©n√©rique si le prompt adaptatif est trop court
            return self._create_prompt(query, context)
        
        except Exception as e:
            # En cas d'erreur, utiliser le prompt g√©n√©rique
            logger.warning(f"Erreur lors de la cr√©ation du prompt adaptatif: {e}")
            return self._create_prompt(query, context)


    def _generate_source_explanation(self, results: List[Dict[str, Any]]) -> str:
        """
        G√©n√®re une explication d√©taill√©e sur l'origine des informations.
        
        Args:
            results: Liste des r√©sultats de recherche
        
        Returns:
            Cha√Æne expliquant l'origine des informations
        """
        if not results:
            return "Aucune source n'a pu √™tre identifi√©e."
        
        # Extraire les informations de source uniques
        sources = {}
        for result in results[:3]:  # Limiter aux 3 premi√®res sources
            metadata = result.get('metadata', {})
            filename = metadata.get('filename', 'Document inconnu')
            page = metadata.get('page_number', '?')
            document_id = metadata.get('document_id', '')
            
            # Utiliser le document_id comme cl√© unique
            if document_id not in sources:
                sources[document_id] = {
                    'filename': filename,
                    'pages': set(),
                    'chunks_count': 0
                }
            sources[document_id]['pages'].add(page)
            sources[document_id]['chunks_count'] += 1
        
        # Construire l'explication
        explanation_parts = []
        for source_details in sources.values():
            pages_str = ', '.join(map(str, sorted(source_details['pages'])))
            explanation_parts.append(
                f"- {source_details['filename']} (pages {pages_str}, "
                f"{source_details['chunks_count']} segment(s) pertinent(s))"
            )
        
        return "Informations extraites de :\n" + "\n".join(explanation_parts)


    def _format_source_documents(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Formate les documents sources pour correspondre au mod√®le attendu.
        G√®re diff√©rents types de r√©sultats, y compris les objets Milvus.
        
        Args:
            results: Liste des r√©sultats de recherche
            
        Returns:
            Liste de documents format√©s
        """
        formatted_docs = []
        
        for result in results:
            # G√©rer les diff√©rents formats de r√©sultat
            try:
                # Si c'est un objet Milvus Hit
                if hasattr(result, 'entity'):
                    # Extraire les informations de l'entit√©
                    entity = result.entity
                    
                    # Initialiser un dictionnaire de r√©sultat
                    formatted_result = {
                        "score": getattr(result, "distance", 0.0),
                        "metadata": {}
                    }
                    
                    # Essayer de r√©cup√©rer le texte
                    text_fields = ['text', 'content', 'data']
                    for field in text_fields:
                        try:
                            text = getattr(entity, field, None)
                            if text:
                                formatted_result["text"] = text
                                break
                        except:
                            continue
                    
                    # R√©cup√©rer les m√©tadonn√©es
                    try:
                        # Essayer de charger les m√©tadonn√©es JSON si possible
                        metadata_str = getattr(entity, "metadata", "{}")
                        if isinstance(metadata_str, str):
                            try:
                                metadata = json.loads(metadata_str)
                            except:
                                metadata = {}
                        elif isinstance(metadata_str, dict):
                            metadata = metadata_str
                        else:
                            metadata = {}
                        
                        # Ajouter d'autres champs de m√©tadonn√©es possibles
                        for field in ['document_id', 'chunk_id', 'filename', 'page_number']:
                            try:
                                value = getattr(entity, field, None)
                                if value:
                                    metadata[field] = value
                            except:
                                pass
                        
                        formatted_result["metadata"] = metadata
                    except Exception as meta_error:
                        logger.warning(f"Erreur lors de l'extraction des m√©tadonn√©es : {meta_error}")
                    
                    # Ajouter √† la liste des documents format√©s
                    formatted_docs.append(formatted_result)
                
                # Si c'est d√©j√† un dictionnaire
                elif isinstance(result, dict):
                    # V√©rifier et formater les documents existants
                    formatted_result = {
                        "text": result.get("text", ""),
                        "score": result.get("score", 0.0),
                        "metadata": result.get("metadata", {})
                    }
                    formatted_docs.append(formatted_result)
                
                # G√©rer d'autres types de r√©sultats potentiels
                else:
                    # Conversion de secours
                    formatted_docs.append({
                        "text": str(result),
                        "score": 0.0,
                        "metadata": {}
                    })
            
            except Exception as e:
                logger.error(f"Erreur lors du formatage d'un document source : {e}")
                # Document de secours en cas d'erreur
                formatted_docs.append({
                    "text": "Erreur de traitement du document",
                    "score": 0.0,
                    "metadata": {"error": str(e)}
                })
        
        return formatted_docs

# Point d'entr√©e principal
if __name__ == "__main__":
    try:
        # Initialiser le syst√®me RAG
        rag_system = RAGSystem()
        
        # Lancer la session interactive
        rag_system.interactive_session()
        
    except Exception as e:
        print(f"Erreur lors de l'initialisation du syst√®me : {e}")